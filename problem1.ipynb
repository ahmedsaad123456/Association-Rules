{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take user inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = input(\"Enter the CSV file name (with .csv extension): \")\n",
    "# 0.2\n",
    "confidence_threshold = float(input(\"Enter the confidence threshold (e.g., 0.20): \"))\n",
    "# 10\n",
    "sup_count = int(input(\"Enter the minimum support count: \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38006\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove rows with duplicate values\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "# Get the number of rows \n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Frequent 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique transactions: 14963\n",
      "167\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "# Create Transaction_ID with the format \"Member_number_Date\"\n",
    "df['Transaction_ID'] = df['Member_number'].astype(str) + \"_\" + df['Date'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Create vertical data format (item → set of transactions)\n",
    "vertical_data = defaultdict(set)\n",
    "\n",
    "\n",
    "# Show the number of the transactions\n",
    "num_transactions = df['Transaction_ID'].nunique()\n",
    "print(\"Number of unique transactions:\", num_transactions)\n",
    "\n",
    "# Create the vertical data \n",
    "for item, txn in zip(df['itemDescription'], df['Transaction_ID']):\n",
    "    vertical_data[item].add(txn)\n",
    "\n",
    "# Convert to dictionary and print sample\n",
    "vertical_data = dict(vertical_data)\n",
    "print(len(vertical_data))\n",
    "\n",
    "# Create a copy from vertical data to use in the confidance calculation\n",
    "one_item_vertical_data = vertical_data.copy()\n",
    "print(len(one_item_vertical_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Candidate 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter vertical data based on support count\n",
    "filtered_vertical_data = {}\n",
    "for item, transactions in vertical_data.items():\n",
    "    if len(transactions) >= sup_count:\n",
    "        filtered_vertical_data[item] = transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# variable to store number of combinations\n",
    "max_k = 2\n",
    "\n",
    "\n",
    "# Create a copy of the filtered vertical data \n",
    "filtered_vertical_data_copy = filtered_vertical_data.copy()\n",
    "\n",
    "\n",
    "\n",
    "isCompleted = True\n",
    "\n",
    "# Loop until no more combinations can be generated\n",
    "while isCompleted:\n",
    "    \n",
    "    # Generate combinations of items based on the current maximum size max_k\n",
    "    all_items = list(filtered_vertical_data.keys())  # List of all item names\n",
    "\n",
    "    # Generate combinations of size max_k\n",
    "\n",
    "    itemsets = combinations(all_items, max_k)  \n",
    "    \n",
    "    \n",
    "\n",
    "    # Create a new map to store the item combinations and their common transactions\n",
    "    new_filtered_vertical_data = {}\n",
    "\n",
    "    # For each item combination, calculate the intersection of transactions\n",
    "    for itemset in itemsets:\n",
    "\n",
    "        # Fetch the transaction sets for each item in the combination\n",
    "        transaction_sets = [filtered_vertical_data[item] for item in itemset]\n",
    "        \n",
    "        # Find the intersection of all transaction sets (common transactions)\n",
    "        common_transactions = set.intersection(*transaction_sets)\n",
    "        \n",
    "\n",
    "        # If the combination greater than support count, add it to the new map\n",
    "        if len(common_transactions) >= sup_count:\n",
    "            new_filtered_vertical_data[itemset] = common_transactions\n",
    "       \n",
    "\n",
    "    # Check if we reached zero length, and if so, restore the previous state\n",
    "    if len(new_filtered_vertical_data) == 0:\n",
    "        filtered_vertical_data = filtered_vertical_data_copy\n",
    "        isCompleted = False\n",
    "        break  \n",
    "    \n",
    "    filtered_vertical_data_copy = new_filtered_vertical_data\n",
    "    \n",
    "    # Increase item set size for the next iteration\n",
    "    max_k += 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tropical fruit', 'whole milk', 'other vegetables')\n",
      "('tropical fruit', 'whole milk', 'rolls/buns')\n",
      "('tropical fruit', 'whole milk', 'yogurt')\n",
      "('tropical fruit', 'whole milk', 'soda')\n",
      "('whole milk', 'pip fruit', 'rolls/buns')\n",
      "('whole milk', 'other vegetables', 'rolls/buns')\n",
      "('whole milk', 'other vegetables', 'frankfurter')\n",
      "('whole milk', 'other vegetables', 'yogurt')\n",
      "('whole milk', 'other vegetables', 'sausage')\n",
      "('whole milk', 'other vegetables', 'root vegetables')\n",
      "('whole milk', 'other vegetables', 'pastry')\n",
      "('whole milk', 'other vegetables', 'soda')\n",
      "('whole milk', 'rolls/buns', 'citrus fruit')\n",
      "('whole milk', 'rolls/buns', 'yogurt')\n",
      "('whole milk', 'rolls/buns', 'sausage')\n",
      "('whole milk', 'rolls/buns', 'pastry')\n",
      "('whole milk', 'rolls/buns', 'canned beer')\n",
      "('whole milk', 'rolls/buns', 'soda')\n",
      "('whole milk', 'rolls/buns', 'bottled beer')\n",
      "('whole milk', 'citrus fruit', 'yogurt')\n",
      "('whole milk', 'bottled water', 'soda')\n",
      "('whole milk', 'yogurt', 'sausage')\n",
      "('whole milk', 'yogurt', 'root vegetables')\n",
      "('whole milk', 'yogurt', 'soda')\n",
      "('whole milk', 'sausage', 'pastry')\n",
      "('whole milk', 'sausage', 'soda')\n",
      "('whole milk', 'pastry', 'soda')\n",
      "('other vegetables', 'rolls/buns', 'sausage')\n",
      "('other vegetables', 'rolls/buns', 'soda')\n",
      "('other vegetables', 'sausage', 'soda')\n",
      "('rolls/buns', 'sausage', 'soda')\n",
      "('rolls/buns', 'soda', 'shopping bags')\n",
      "('yogurt', 'sausage', 'soda')\n"
     ]
    }
   ],
   "source": [
    "for key in filtered_vertical_data.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and print The strong association rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strong Association Rules:\n",
      "Rule: {'rolls/buns', 'sausage'} => {'whole milk'}, SupportAll: 17, Support : 80 ,Confidence: 0.2125\n",
      "Rule: {'yogurt', 'sausage'} => {'whole milk'}, SupportAll: 22, Support : 86 ,Confidence: 0.2558139534883721\n",
      "Rule: {'pastry', 'sausage'} => {'whole milk'}, SupportAll: 11, Support : 48 ,Confidence: 0.22916666666666666\n",
      "Rule: {'pastry', 'soda'} => {'whole milk'}, SupportAll: 14, Support : 61 ,Confidence: 0.22950819672131148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to generate all non-empty subsets of a set\n",
    "def get_subsets(itemset):\n",
    "    subsets = []\n",
    "    for i in range(1, len(itemset)):\n",
    "        subsets.extend(combinations(itemset, i))\n",
    "    return subsets\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate support for a given itemset\n",
    "def calculate_support(itemset, filtered_vertical_data):\n",
    "    support_count = 0\n",
    "    # Fetch the transaction sets for each item in the combination\n",
    "    transaction_sets = [filtered_vertical_data[item] for item in itemset]\n",
    "\n",
    "    \n",
    "    # Find the intersection of all transaction sets (common transactions)\n",
    "    common_transactions = set.intersection(*transaction_sets)\n",
    "    \n",
    "    # If the combination has common transactions, add it to the new map\n",
    "    if len(common_transactions) > 0:\n",
    "        support_count += len(common_transactions)\n",
    "\n",
    "        \n",
    "       \n",
    "    return support_count\n",
    "\n",
    "\n",
    "# Generate strong association rules\n",
    "def generate_association_rules(filtered_vertical_data):\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over all itemsets (frequent itemsets)\n",
    "    for itemset, transactions in filtered_vertical_data.items():\n",
    "        itemset_size = len(itemset)\n",
    "        \n",
    "        if itemset_size > 1:  \n",
    "\n",
    "            # Get all non-empty subsets of the itemset\n",
    "            subsets = get_subsets(itemset)\n",
    "            \n",
    "            for subset in subsets:\n",
    "                # X is the subset and Y is the complement (rest of the items in the itemset)\n",
    "                X = set(subset)\n",
    "                Y = set(itemset) - X\n",
    "                \n",
    "                # Calculate support for X and X ∪ Y\n",
    "                # For X, count how many transactions contain all items in X\n",
    "                # and take the one_item vertical data \n",
    "                support_X = calculate_support(X, one_item_vertical_data)\n",
    "                # Support for the entire itemset (X ∪ Y)\n",
    "                support_XY = len(transactions)  \n",
    "                # Calculate confidence\n",
    "                confidence = support_XY / support_X if support_X > 0 else 0\n",
    "                \n",
    "                # If confidence is greater than or equal to the threshold, we have a strong rule\n",
    "                if confidence >= confidence_threshold:\n",
    "                    rule = {\n",
    "                        'Rule': f\"{X} => {Y}\",\n",
    "                        'SupportAll': support_XY,\n",
    "                        'Support' : support_X,\n",
    "                        'Confidence': confidence\n",
    "                    }\n",
    "                    rules.append(rule)\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Now generate the strong association rules from filtered_vertical_data\n",
    "strong_rules = generate_association_rules(filtered_vertical_data)\n",
    "\n",
    "# Print the strong association rules\n",
    "print(\"\\nStrong Association Rules:\")\n",
    "for rule in strong_rules:\n",
    "    print(f\"Rule: {rule['Rule']}, SupportAll: {rule['SupportAll']}, Support : {rule['Support']} ,Confidence: {rule['Confidence']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
