{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take user inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = input(\"Enter the CSV file name (with .csv extension): \")\n",
    "# 0.2\n",
    "confidence_threshold = float(input(\"Enter the confidence threshold (e.g., 0.20): \"))\n",
    "# 10\n",
    "sup_count = int(input(\"Enter the minimum support count: \"))\n",
    "\n",
    "percentage = float(input(\"Enter the percentage of the dataset to process (e.g., 50 for 50%): \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "# Ensure the percentage is valid\n",
    "if 0 < percentage <= 100:\n",
    "    df = df.sample(frac=percentage / 100, random_state=42)  \n",
    "else:\n",
    "    print(\"Invalid percentage! Using the full dataset.\")\n",
    "\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove rows with duplicate values\n",
    "df = df.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Frequent 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique transactions: 14963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "# Create Transaction_ID with the format \"Member_number_Date\"\n",
    "df['Transaction_ID'] = df['Member_number'].astype(str) + \"_\" + df['Date'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "# Create vertical data format (item → set of transactions)\n",
    "vertical_data = defaultdict(set)\n",
    "\n",
    "\n",
    "# Show the number of the transactions\n",
    "num_transactions = df['Transaction_ID'].nunique()\n",
    "print(\"Number of unique transactions:\", num_transactions)\n",
    "\n",
    "# Create the vertical data \n",
    "for item, txn in zip(df['itemDescription'], df['Transaction_ID']):\n",
    "    vertical_data[item].add(txn)\n",
    "\n",
    "# Convert to dictionary and print sample\n",
    "vertical_data = dict(vertical_data)\n",
    "\n",
    "# Create a copy from vertical data to use in the confidance calculation\n",
    "one_item_vertical_data = vertical_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Candidate 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter vertical data based on support count\n",
    "filtered_vertical_data = {}\n",
    "for item, transactions in vertical_data.items():\n",
    "    if len(transactions) >= sup_count:\n",
    "        filtered_vertical_data[item] = transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize max_k for itemset sizes\n",
    "max_k = 2  \n",
    "\n",
    "# Store previous frequent itemsets\n",
    "filtered_vertical_data_copy = filtered_vertical_data.copy()\n",
    "\n",
    "# print(\"Initial Frequent Itemsets:\", filtered_vertical_data_copy)\n",
    "\n",
    "# Continue generating itemsets until no more frequent sets are found\n",
    "while True:\n",
    "    prev_frequent_itemsets = list(filtered_vertical_data_copy.keys())\n",
    "\n",
    "    # print(f\"\\n=== Iteration {max_k-1} ===\")\n",
    "\n",
    "    # Stop if there are no more frequent itemsets\n",
    "    if len(prev_frequent_itemsets) < 2:\n",
    "        # print(\"No more frequent itemsets. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # Generate candidate itemsets of size max_k from previous (max_k-1) itemsets\n",
    "    new_filtered_vertical_data = {}\n",
    "\n",
    "    for a, b in combinations(prev_frequent_itemsets, 2):\n",
    "\n",
    "        # Ensure merging happens correctly\n",
    "        # Assuming `a` and `b` are two items to be merged\n",
    "        if isinstance(a, tuple) and isinstance(b, tuple):\n",
    "            # Merge and sort\n",
    "            merged_set = sorted(set(a) | set(b))  \n",
    "        else:\n",
    "            # If not tuples, keep them as is\n",
    "            merged_set = (a, b)  \n",
    "\n",
    "        # convert merged_set to a tuple\n",
    "        merged_set = tuple(merged_set)\n",
    "        \n",
    "        # Only merge sets that differ by one item\n",
    "        if len(merged_set) == max_k:\n",
    "\n",
    "            # Merge transaction sets from previous frequent itemsets\n",
    "            common_transactions = filtered_vertical_data_copy[a] & filtered_vertical_data_copy[b]\n",
    "\n",
    "\n",
    "            # Store only frequent itemsets\n",
    "            if len(common_transactions) >= sup_count:\n",
    "                new_filtered_vertical_data[merged_set] = common_transactions\n",
    "\n",
    "    # Stop if no new frequent itemsets are found\n",
    "    if not new_filtered_vertical_data:\n",
    "        break\n",
    "\n",
    "    # Update for the next iteration\n",
    "    filtered_vertical_data_copy = new_filtered_vertical_data\n",
    "    max_k += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('rolls/buns', 'tropical fruit', 'whole milk')\n",
      "('soda', 'tropical fruit', 'whole milk')\n",
      "('other vegetables', 'tropical fruit', 'whole milk')\n",
      "('tropical fruit', 'whole milk', 'yogurt')\n",
      "('frankfurter', 'other vegetables', 'whole milk')\n",
      "('pastry', 'rolls/buns', 'whole milk')\n",
      "('pastry', 'soda', 'whole milk')\n",
      "('pastry', 'sausage', 'whole milk')\n",
      "('other vegetables', 'pastry', 'whole milk')\n",
      "('canned beer', 'rolls/buns', 'whole milk')\n",
      "('rolls/buns', 'soda', 'whole milk')\n",
      "('rolls/buns', 'sausage', 'whole milk')\n",
      "('pip fruit', 'rolls/buns', 'whole milk')\n",
      "('other vegetables', 'rolls/buns', 'whole milk')\n",
      "('rolls/buns', 'whole milk', 'yogurt')\n",
      "('bottled beer', 'rolls/buns', 'whole milk')\n",
      "('citrus fruit', 'rolls/buns', 'whole milk')\n",
      "('rolls/buns', 'sausage', 'soda')\n",
      "('other vegetables', 'rolls/buns', 'soda')\n",
      "('rolls/buns', 'shopping bags', 'soda')\n",
      "('other vegetables', 'rolls/buns', 'sausage')\n",
      "('sausage', 'soda', 'whole milk')\n",
      "('bottled water', 'soda', 'whole milk')\n",
      "('other vegetables', 'soda', 'whole milk')\n",
      "('soda', 'whole milk', 'yogurt')\n",
      "('other vegetables', 'sausage', 'whole milk')\n",
      "('sausage', 'whole milk', 'yogurt')\n",
      "('other vegetables', 'whole milk', 'yogurt')\n",
      "('other vegetables', 'root vegetables', 'whole milk')\n",
      "('citrus fruit', 'whole milk', 'yogurt')\n",
      "('root vegetables', 'whole milk', 'yogurt')\n",
      "('other vegetables', 'sausage', 'soda')\n",
      "('sausage', 'soda', 'yogurt')\n"
     ]
    }
   ],
   "source": [
    "for key in filtered_vertical_data_copy.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and print The strong association rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strong Association Rules:\n",
      "Rule: {'pastry', 'soda'} => {'whole milk'}, SupportAll: 14, Support : 61 ,Confidence: 0.22950819672131148\n",
      "Rule: {'pastry', 'sausage'} => {'whole milk'}, SupportAll: 11, Support : 48 ,Confidence: 0.22916666666666666\n",
      "Rule: {'sausage', 'rolls/buns'} => {'whole milk'}, SupportAll: 17, Support : 80 ,Confidence: 0.2125\n",
      "Rule: {'sausage', 'yogurt'} => {'whole milk'}, SupportAll: 22, Support : 86 ,Confidence: 0.2558139534883721\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to generate all non-empty subsets of a set\n",
    "def get_subsets(itemset):\n",
    "    subsets = []\n",
    "    for i in range(1, len(itemset)):\n",
    "        subsets.extend(combinations(itemset, i))\n",
    "    return subsets\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate support for a given itemset\n",
    "def calculate_support(itemset, filtered_vertical_data):\n",
    "    support_count = 0\n",
    "    # Fetch the transaction sets for each item in the combination\n",
    "    transaction_sets = [filtered_vertical_data[item] for item in itemset]\n",
    "\n",
    "    \n",
    "    # Find the intersection of all transaction sets (common transactions)\n",
    "    common_transactions = set.intersection(*transaction_sets)\n",
    "    \n",
    "    # If the combination has common transactions, add it to the new map\n",
    "    if len(common_transactions) > 0:\n",
    "        support_count += len(common_transactions)\n",
    "\n",
    "        \n",
    "       \n",
    "    return support_count\n",
    "\n",
    "\n",
    "# Generate strong association rules\n",
    "def generate_association_rules(filtered_vertical_data):\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over all itemsets (frequent itemsets)\n",
    "    for itemset, transactions in filtered_vertical_data.items():\n",
    "        itemset_size = len(itemset)\n",
    "        \n",
    "        if itemset_size > 1:  \n",
    "\n",
    "            # Get all non-empty subsets of the itemset\n",
    "            subsets = get_subsets(itemset)\n",
    "            \n",
    "            for subset in subsets:\n",
    "                # X is the subset and Y is the complement (rest of the items in the itemset)\n",
    "                X = set(subset)\n",
    "                Y = set(itemset) - X\n",
    "                \n",
    "                # Calculate support for X and X ∪ Y\n",
    "                # For X, count how many transactions contain all items in X\n",
    "                # and take the one_item vertical data \n",
    "                support_X = calculate_support(X, one_item_vertical_data)\n",
    "                # Support for the entire itemset (X ∪ Y)\n",
    "                support_XY = len(transactions)  \n",
    "                # Calculate confidence\n",
    "                confidence = support_XY / support_X if support_X > 0 else 0\n",
    "                \n",
    "                # If confidence is greater than or equal to the threshold, we have a strong rule\n",
    "                if confidence >= confidence_threshold:\n",
    "                    rule = {\n",
    "                        'Rule': f\"{X} => {Y}\",\n",
    "                        'SupportAll': support_XY,\n",
    "                        'Support' : support_X,\n",
    "                        'Confidence': confidence\n",
    "                    }\n",
    "                    rules.append(rule)\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Now generate the strong association rules from filtered_vertical_data\n",
    "strong_rules = generate_association_rules(filtered_vertical_data_copy)\n",
    "\n",
    "# Print the strong association rules\n",
    "print(\"\\nStrong Association Rules:\")\n",
    "for rule in strong_rules:\n",
    "    print(f\"Rule: {rule['Rule']}, SupportAll: {rule['SupportAll']}, Support : {rule['Support']} ,Confidence: {rule['Confidence']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
