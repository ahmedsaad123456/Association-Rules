{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take user inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = input(\"Enter the txt file name (with .txt extension): \")\n",
    "# 0.5\n",
    "confidence_threshold = float(input(\"Enter the confidence threshold (e.g., 0.20): \"))\n",
    "#1000\n",
    "sup_count = int(input(\"Enter the minimum support count: \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Categories\n",
      "0      Breakfast & Brunch;American (Traditional);Rest...\n",
      "1                                 Sandwiches;Restaurants\n",
      "2           Local Services;IT Services & Computer Repair\n",
      "3                                    Restaurants;Italian\n",
      "4                                      Food;Coffee & Tea\n",
      "...                                                  ...\n",
      "75813  Doctors;Cosmetic Surgeons;Beauty & Spas;Medica...\n",
      "75814                                   Fashion;Shopping\n",
      "75815  Doctors;Health & Medical;Obstetricians & Gynec...\n",
      "75816                                  Food;Coffee & Tea\n",
      "75817         Food;Health Markets;Grocery;Specialty Food\n",
      "\n",
      "[75818 rows x 1 columns]\n",
      "75818\n"
     ]
    }
   ],
   "source": [
    "# Define the header as None beause the file has no header\n",
    "# The file has only one column with the name \"Categories\"\n",
    "# skip_blank_lines=True will skip any blank lines\n",
    "# on_bad_lines='skip' will skip any bad lines that cannot be parsed\n",
    "\n",
    "df = pd.read_csv(file_name, header=None, names=[\"Categories\"], skip_blank_lines=True , on_bad_lines='skip')\n",
    "\n",
    "print(df)\n",
    "# Get the number of rows \n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Frequent 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique transactions: 75818\n",
      "Number of unique categories: 882\n"
     ]
    }
   ],
   "source": [
    "# Create a unique transaction ID for each row\n",
    "# Using row index as unique transaction ID\n",
    "# Converts MultiIndex into regular columns\n",
    "df = df.reset_index()  \n",
    "# Assign unique transaction ID\n",
    "df[\"Transaction_ID\"] = df.index.astype(str)  \n",
    "\n",
    "# Create vertical data format (category → set of transactions)\n",
    "vertical_data = defaultdict(set)\n",
    "\n",
    "# Show the number of transactions (each line is a transaction)\n",
    "num_transactions = len(df)\n",
    "print(\"Number of unique transactions:\", num_transactions)\n",
    "\n",
    "# Populate vertical data with category → transaction mapping\n",
    "for txn_id, categories in zip(df[\"Transaction_ID\"], df[\"Categories\"]):\n",
    "    category_list = categories.split(\";\")  # Split categories by semicolon\n",
    "    for category in category_list:\n",
    "        # Strip spaces for consistency\n",
    "        vertical_data[category.strip()].add(txn_id)  \n",
    "\n",
    "# Convert defaultdict to a regular dictionary\n",
    "vertical_data = dict(vertical_data)\n",
    "print(\"Number of unique categories:\", len(vertical_data))\n",
    "\n",
    "# Create a copy of vertical data for confidence calculations\n",
    "one_item_vertical_data = vertical_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Candidate 1 itemset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter vertical data based on support count\n",
    "filtered_vertical_data = {}\n",
    "for item, transactions in vertical_data.items():\n",
    "    if len(transactions) >= sup_count:\n",
    "        filtered_vertical_data[item] = transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize max_k for itemset sizes\n",
    "max_k = 2  \n",
    "\n",
    "# Store previous frequent itemsets\n",
    "filtered_vertical_data_copy = filtered_vertical_data.copy()\n",
    "\n",
    "# print(\"Initial Frequent Itemsets:\", filtered_vertical_data_copy)\n",
    "\n",
    "# Continue generating itemsets until no more frequent sets are found\n",
    "while True:\n",
    "    prev_frequent_itemsets = list(filtered_vertical_data_copy.keys())\n",
    "\n",
    "    # print(f\"\\n=== Iteration {max_k-1} ===\")\n",
    "\n",
    "    # Stop if there are no more frequent itemsets\n",
    "    if len(prev_frequent_itemsets) < 2:\n",
    "        # print(\"No more frequent itemsets. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # Generate candidate itemsets of size max_k from previous (max_k-1) itemsets\n",
    "    new_filtered_vertical_data = {}\n",
    "\n",
    "    for a, b in combinations(prev_frequent_itemsets, 2):\n",
    "\n",
    "        # Ensure merging happens correctly\n",
    "        # Assuming `a` and `b` are two items to be merged\n",
    "        if isinstance(a, tuple) and isinstance(b, tuple):\n",
    "            # Merge and sort\n",
    "            merged_set = sorted(set(a) | set(b))  \n",
    "        else:\n",
    "            # If not tuples, keep them as is\n",
    "            merged_set = (a, b)  \n",
    "\n",
    "        # convert merged_set to a tuple\n",
    "        merged_set = tuple(merged_set)\n",
    "        \n",
    "        # Only merge sets that differ by one item\n",
    "        if len(merged_set) == max_k:\n",
    "\n",
    "            # Merge transaction sets from previous frequent itemsets\n",
    "            common_transactions = filtered_vertical_data_copy[a] & filtered_vertical_data_copy[b]\n",
    "\n",
    "\n",
    "            # Store only frequent itemsets\n",
    "            if len(common_transactions) >= sup_count:\n",
    "                new_filtered_vertical_data[merged_set] = common_transactions\n",
    "\n",
    "    # Stop if no new frequent itemsets are found\n",
    "    if not new_filtered_vertical_data:\n",
    "        break\n",
    "\n",
    "    # Update for the next iteration\n",
    "    filtered_vertical_data_copy = new_filtered_vertical_data\n",
    "    max_k += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print The frequent item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bars', 'Nightlife', 'Restaurants')\n",
      "('Event Planning & Services', 'Hotels', 'Hotels & Travel')\n",
      "('Fashion', 'Shopping', \"Women's Clothing\")\n"
     ]
    }
   ],
   "source": [
    "for key in filtered_vertical_data_copy.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and print The strong association rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strong Association Rules:\n",
      "Rule: {'Bars'} => {'Restaurants', 'Nightlife'}, SupportAll: 2399, Support : 4269 ,Confidence: 0.5619583040524713\n",
      "Rule: {'Bars', 'Nightlife'} => {'Restaurants'}, SupportAll: 2399, Support : 4269 ,Confidence: 0.5619583040524713\n",
      "Rule: {'Bars', 'Restaurants'} => {'Nightlife'}, SupportAll: 2399, Support : 2399 ,Confidence: 1.0\n",
      "Rule: {'Nightlife', 'Restaurants'} => {'Bars'}, SupportAll: 2399, Support : 2507 ,Confidence: 0.9569206222576785\n",
      "Rule: {'Hotels'} => {'Hotels & Travel', 'Event Planning & Services'}, SupportAll: 1431, Support : 1431 ,Confidence: 1.0\n",
      "Rule: {'Hotels & Travel'} => {'Event Planning & Services', 'Hotels'}, SupportAll: 1431, Support : 2492 ,Confidence: 0.5742375601926164\n",
      "Rule: {'Event Planning & Services', 'Hotels'} => {'Hotels & Travel'}, SupportAll: 1431, Support : 1431 ,Confidence: 1.0\n",
      "Rule: {'Hotels & Travel', 'Event Planning & Services'} => {'Hotels'}, SupportAll: 1431, Support : 1471 ,Confidence: 0.972807613868117\n",
      "Rule: {'Hotels & Travel', 'Hotels'} => {'Event Planning & Services'}, SupportAll: 1431, Support : 1431 ,Confidence: 1.0\n",
      "Rule: {\"Women's Clothing\"} => {'Fashion', 'Shopping'}, SupportAll: 1091, Support : 1091 ,Confidence: 1.0\n",
      "Rule: {'Fashion', \"Women's Clothing\"} => {'Shopping'}, SupportAll: 1091, Support : 1091 ,Confidence: 1.0\n",
      "Rule: {\"Women's Clothing\", 'Shopping'} => {'Fashion'}, SupportAll: 1091, Support : 1091 ,Confidence: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to generate all non-empty subsets of a set\n",
    "def get_subsets(itemset):\n",
    "    subsets = []\n",
    "    for i in range(1, len(itemset)):\n",
    "        subsets.extend(combinations(itemset, i))\n",
    "    return subsets\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate support for a given itemset\n",
    "def calculate_support(itemset, filtered_vertical_data):\n",
    "    support_count = 0\n",
    "    # Fetch the transaction sets for each item in the combination\n",
    "    transaction_sets = [filtered_vertical_data[item] for item in itemset]\n",
    "\n",
    "    \n",
    "    # Find the intersection of all transaction sets (common transactions)\n",
    "    common_transactions = set.intersection(*transaction_sets)\n",
    "    \n",
    "    # If the combination has common transactions, add it to the new map\n",
    "    if len(common_transactions) > 0:\n",
    "        support_count += len(common_transactions)\n",
    "\n",
    "        \n",
    "       \n",
    "    return support_count\n",
    "\n",
    "\n",
    "# Generate strong association rules\n",
    "def generate_association_rules(filtered_vertical_data):\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over all itemsets (frequent itemsets)\n",
    "    for itemset, transactions in filtered_vertical_data.items():\n",
    "        itemset_size = len(itemset)\n",
    "        \n",
    "        if itemset_size > 1:  \n",
    "\n",
    "            # Get all non-empty subsets of the itemset\n",
    "            subsets = get_subsets(itemset)\n",
    "            \n",
    "            for subset in subsets:\n",
    "                # X is the subset and Y is the complement (rest of the items in the itemset)\n",
    "                X = set(subset)\n",
    "                Y = set(itemset) - X\n",
    "                \n",
    "                # Calculate support for X and X ∪ Y\n",
    "                # For X, count how many transactions contain all items in X\n",
    "                # and take the one_item vertical data \n",
    "                support_X = calculate_support(X, one_item_vertical_data)\n",
    "                # Support for the entire itemset (X ∪ Y)\n",
    "                support_XY = len(transactions)  \n",
    "                # Calculate confidence\n",
    "                confidence = support_XY / support_X if support_X > 0 else 0\n",
    "                \n",
    "                # If confidence is greater than or equal to the threshold, we have a strong rule\n",
    "                if confidence >= confidence_threshold:\n",
    "                    rule = {\n",
    "                        'Rule': f\"{X} => {Y}\",\n",
    "                        'SupportAll': support_XY,\n",
    "                        'Support' : support_X,\n",
    "                        'Confidence': confidence\n",
    "                    }\n",
    "                    rules.append(rule)\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Now generate the strong association rules from filtered_vertical_data\n",
    "strong_rules = generate_association_rules(filtered_vertical_data_copy)\n",
    "\n",
    "# Print the strong association rules\n",
    "print(\"\\nStrong Association Rules:\")\n",
    "for rule in strong_rules:\n",
    "    print(f\"Rule: {rule['Rule']}, SupportAll: {rule['SupportAll']}, Support : {rule['Support']} ,Confidence: {rule['Confidence']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
